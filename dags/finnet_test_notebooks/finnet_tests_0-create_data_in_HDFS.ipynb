{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create data for fullchain tests\n",
    "\n",
    "See: finnet-pipeline/docker-tests/fullchain/create_data.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"python2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init(\"/usr/local/spark\")\n",
    "\n",
    "import sys\n",
    "from operator import add\n",
    "\n",
    "from pyspark.sql import HiveContext, SQLContext, SparkSession\n",
    "from pyspark import SparkContext, SparkConf\n",
    "sc = SparkContext.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_context = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"/usr/local/dags\")\n",
    "\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "from fncore.utils.graph_specification import GraphSpec\n",
    "from fncore.utils.spark_tools import get_spark_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"/datasets/finnet\"\n",
    "DATA_FORMAT = \"parquet\"\n",
    "LOCAL_DATA_PATH = os.path.join(os.getcwd(), \"data\")\n",
    "# os.environ[\"PYSPARK_SUBMIT_ARGS\"] = \\\n",
    "#      \"--packages com.databricks:spark-csv_2.11:1.4.0 \" \\\n",
    "#      \"pyspark-shell\"\n",
    "\n",
    "\n",
    "# # Setup the spark configuration\n",
    "# CONFIG = dict()\n",
    "# CONFIG[\"SparkConfiguration\"] = (SparkConf()\n",
    "#                                 .setMaster(\"local\")\n",
    "#                                 .setAppName(\"test create data\")\n",
    "#                                 .set(\"spark.executor.memory\", \"512m\"))\n",
    "\n",
    "os.environ[\"GRAPH_DB\"] = \"\"\"bolt://neo4j:test@neo4j:7687\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get graph specs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = os.listdir(LOCAL_DATA_PATH)\n",
    "json_list = [k for k in data_list if re.match(r'.*\\.json$', k)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the graph spec\n",
    "for graph_spec in json_list:\n",
    "    with open(os.path.join(LOCAL_DATA_PATH, graph_spec), 'r') as f:\n",
    "        spec = json.load(f)\n",
    "        spec_model = GraphSpec.from_dict(spec)\n",
    "\n",
    "    tables = spec_model.table_details\n",
    "    graph_name = spec_model.name\n",
    "\n",
    "    # Read the sample data and put into hdfs\n",
    "    for table, columns in tables['tables'].items():\n",
    "        source_table, safe_table = table\n",
    "        filepath = 'file://' + \\\n",
    "                   os.path.join(LOCAL_DATA_PATH, str(source_table)) + \\\n",
    "                   '.csv'\n",
    "        data = sql_context.read.format('com.databricks.spark.csv')\\\n",
    "                          .option('header', 'true')\\\n",
    "                          .option('inferschema', 'false')\\\n",
    "                          .load(filepath)\n",
    "\n",
    "        outdatapath = os.path.join(\n",
    "            DATA_PATH, graph_name, 'tables', safe_table\n",
    "        )\n",
    "        data.write.format(DATA_FORMAT)\\\n",
    "            .mode(saveMode='overwrite')\\\n",
    "            .save(outdatapath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finnet-pipeline (Python 2)",
   "language": "python",
   "name": "finnet-pipeline"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
